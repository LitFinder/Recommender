{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/regy/miniconda3/envs/tf/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-06-11 19:58:13.236374: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-06-11 19:58:13.982898: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/regy/miniconda3/envs/tf/lib/\n",
      "2024-06-11 19:58:13.983018: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: :/home/regy/miniconda3/envs/tf/lib/\n",
      "2024-06-11 19:58:13.983027: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/regy/miniconda3/envs/tf/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (693 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average token length: 198.1050326073582\n",
      "Maximum token length: 2061\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('data_book_metadata.csv')\n",
    "\n",
    "# Extract the sentences from the Metadata column\n",
    "sentences = df['Metadata'].tolist()\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Tokenize the sentences without truncation to get their lengths\n",
    "token_lengths = [len(tokenizer.encode(sentence, add_special_tokens=True)) for sentence in sentences]\n",
    "\n",
    "# Calculate the average and maximum token lengths\n",
    "average_token_length = np.mean(token_lengths)\n",
    "max_token_length = np.max(token_lengths)\n",
    "\n",
    "print(f'Average token length: {average_token_length}')\n",
    "print(f'Maximum token length: {max_token_length}')\n",
    "\n",
    "# Tokenize the sentences with truncation to max_length of 512\n",
    "inputs = tokenizer(sentences, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "\n",
    "# Extract input_ids and attention_mask\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, input_ids, attention_mask, targets):\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {\n",
    "            'input_ids': self.input_ids[idx],\n",
    "            'attention_mask': self.attention_mask[idx],\n",
    "            'targets': self.targets[idx]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate random target embeddings for demonstration purposes (replace with actual targets)\n",
    "target_embeddings = np.random.rand(len(sentences), 256).astype(np.float32)\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "train_inputs, val_inputs, train_targets, val_targets = train_test_split(input_ids, target_embeddings, test_size=0.1, random_state=42)\n",
    "train_masks, val_masks = train_test_split(attention_mask, test_size=0.1, random_state=42)\n",
    "\n",
    "# Create PyTorch datasets\n",
    "train_dataset = TextDataset(train_inputs, train_masks, train_targets)\n",
    "val_dataset = TextDataset(val_inputs, val_masks, val_targets)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the pre-trained model\n",
    "model = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')\n",
    "\n",
    "# Define the fine-tuning model\n",
    "class FineTuningModel(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super(FineTuningModel, self).__init__()\n",
    "        self.base_model = base_model\n",
    "        self.dense = nn.Linear(384, 256)  # 384 is the hidden size of all-MiniLM-L6-v2\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.base_model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "        cls_token = last_hidden_state[:, 0, :]\n",
    "        dense_output = self.dense(cls_token)\n",
    "        return dense_output\n",
    "\n",
    "# Initialize the fine-tuning model\n",
    "fine_tuned_model = FineTuningModel(model)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.SGD(fine_tuned_model.parameters(), lr=1e-4, momentum=0.9)\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        optimizer.zero_grad()\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        targets = batch['targets'].to(device)\n",
    "\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate_epoch(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            targets = batch['targets'].to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, targets)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9143/9143 [16:05<00:00,  9.47it/s]\n",
      "Evaluating: 100%|██████████| 1016/1016 [00:34<00:00, 29.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2, Train Loss: 0.10871803536758529, Val Loss: 0.08430421510784644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 9143/9143 [16:02<00:00,  9.50it/s]\n",
      "Evaluating: 100%|██████████| 1016/1016 [00:34<00:00, 29.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/2, Train Loss: 0.08619009844455992, Val Loss: 0.08381352628632559\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "fine_tuned_model.to(device)\n",
    "\n",
    "epochs = 2\n",
    "best_val_loss = float('inf')\n",
    "early_stopping_patience = 5\n",
    "early_stopping_counter = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss = train_epoch(fine_tuned_model, train_dataloader, optimizer, criterion, device)\n",
    "    val_loss = evaluate_epoch(fine_tuned_model, val_dataloader, criterion, device)\n",
    "    print(f'Epoch {epoch + 1}/{epochs}, Train Loss: {train_loss}, Val Loss: {val_loss}')\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        early_stopping_counter = 0\n",
    "        torch.save(fine_tuned_model.state_dict(), 'fine_tuned_all_mini_lm_l6_v2.pt')\n",
    "    else:\n",
    "        early_stopping_counter += 1\n",
    "\n",
    "    if early_stopping_counter >= early_stopping_patience:\n",
    "        print(\"Early stopping\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(fine_tuned_model.state_dict(), 'fine_tuned_embedding.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(fine_tuned_model, 'fine_tuned_embedding')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Function to convert state dictionary to a JSON-compatible format\n",
    "def state_dict_to_json_compatible(state_dict):\n",
    "    json_compatible_state_dict = {}\n",
    "    for key, value in state_dict.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            json_compatible_state_dict[key] = value.tolist()\n",
    "        else:\n",
    "            json_compatible_state_dict[key] = value\n",
    "    return json_compatible_state_dict\n",
    "\n",
    "# Function to save the model's state dictionary to a JSON file\n",
    "def save_model_to_json(model, file_path):\n",
    "    state_dict = model.state_dict()\n",
    "    json_compatible_state_dict = state_dict_to_json_compatible(state_dict)\n",
    "    \n",
    "    with open(file_path, 'w') as f:\n",
    "        json.dump(json_compatible_state_dict, f)\n",
    "\n",
    "# Save the model's state dictionary to a JSON file\n",
    "save_model_to_json(fine_tuned_model, 'fine_tune_embedding.json')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
